# -*- coding: utf-8 -*-
"""CA4015-Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RoNLsT0l6cgTDMKkx-fdrv1fC_IyBg6U

Student name: Aoife McDaid

Student number: 19369973

# Method 1: Using a Random Forest

## Loading the data
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

df = pd.read_csv('/content/diabetes.csv') #loading dataset

df.head(5)

# Plotting histograms to get a better visual of the data
df.hist(figsize=(12,12))

"""## Cleaning the data"""

#checking for any missing values
df.info()

print(df.isnull().sum())

"""The two tables above show that there are no missing values in the data."""

df.describe()

"""From the table above, I am particularily interested in the columns where the min is zero. Zero should not be seen when recording Glucose, BloodPressure, SkinThickness, Insulin and BMI. I need to replace these zero values with the mean or median of the particular column.

"""

# Decide whether to use mean or median based on if the data is skewed
skewValue = df.skew(axis=1)
print(skewValue)

"""Above it is clear the data is skewed. Due to this I will replace the zero values with the median of the column."""

#Cannot just replace all zeros with NaN as zeros shown in the pregnancy column are not errors.
col_err = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[col_err] = df[col_err].replace(0, np.NaN)

# Checking to see if zero values have successfully been relplaced by NaN.
df.isnull().sum()

#Fill the NaN values with the median
input = SimpleImputer(missing_values = np.NaN, strategy='median')
df[col_err] = input.fit_transform(df[col_err])

#checking to see if NaN values have been replaced by the median.
df.isnull().sum()

df.describe()

"""In the table above we can see that the min is no longer zero for the particular columns.

Below is histogram plots after replacing zero values with the median value of the column
"""

df.hist(figsize = (12,12))

"""Next I am going to check whether there is imbalance in my data."""

labels = df['Outcome'] # labels are the values we want to predict
features = df.drop('Outcome', axis = 1) # remove outcomes from the dataset

labels.value_counts()

"""Above it is clear that the data is imbalance and therefore we will need to balance it."""

from imblearn.over_sampling import SMOTE
features, labels = SMOTE().fit_resample(features, labels)
labels.value_counts()

"""Above we can see the data has been balanced.

## Split the data into train and test
"""

# Data being split train-80% and test-20%
train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.2, random_state = 0)

#Examining shape of the train/test data
print('Training Features Shape:', train_features.shape)
print('Training Labels Shape:', train_labels.shape)
print('Testing Features Shape:', test_features.shape)
print('Testing Labels Shape:', test_labels.shape)

"""## Scaling the model

Normalize the features using StandardScaler
"""

from sklearn.preprocessing import StandardScaler
train_features = StandardScaler().fit_transform(train_features)
test_features = StandardScaler().fit_transform(test_features)

"""## Training the model using Random Forest Classifier

I decided to use the RandomForestClassifier() function as my goal is to determine which class the target belongs in, either 0 or 1.
"""

#Introducing Random Forest Classifier
classifier = RandomForestClassifier()
classifier.fit(train_features, train_labels)
prediction = classifier.predict(test_features)

"""## Evaluating the Algorithm"""

print('Confusion Matrix:')
print(confusion_matrix(test_labels,prediction))
print('Classification Report:')
print(classification_report(test_labels,prediction))
print('Accuracy Score:', accuracy_score(test_labels, prediction))

"""# Method 2: Using an automated ML method via PyCaret

## Loading the diabetes dataset
"""

dataframe = pd.read_csv('/content/diabetes.csv') #loading dataset

dataframe.head()

# Replace zeros with NaN in columns: Glucose, BloodPressure, SkinThickness, Insulin and BMI
col_err = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
dataframe[col_err] = dataframe[col_err].replace(0, np.NaN)
#Fill the NaN values with the median
#input = SimpleImputer(missing_values = np.NaN, strategy='median')
#dataframe[col_err] = input.fit_transform(dataframe[col_err])

"""## Installing PyCaret and preparing the data"""

pip install pycaret

from pycaret.classification import *

setup = setup(data=dataframe, target='Outcome', numeric_imputation='median', remove_outliers=True, fix_imbalance=True, normalize=True)

"""## Training the Models

Using PyCaret library you are able to run any machine learning model at the same time. The compare_models() function trains and evaluates the performance of all methods available in the library.
"""

best_model = compare_models()

"""From the result we can see the models and their performance listed in order based on the accuracy. 

Although the table shows that Light Gradient Boosting machine has the highest accuracy, it is only the highest in three of the other columns; Precision, Kappa and MCC. Gradient Boosting Classifier is listed second in relation to accuracy however it is not the highest in any other column, wherelse Logistic Regression (which is ranked third in relation to accuracy) has the highest AUC, Recall and F1 scores. This can be seen more clearly below.
"""

# Comparing Light Gradient Boosting, Gradient Boosting Classifier and Logistic Regression
top_models = compare_models(include = ['lightgbm','gbc', 'lr'])

LGBM = create_model('lightgbm')

#Interested to look at the Random Forest method after the previous work done
random_forest = compare_models(include = ['rf'])

"""From this we can tune the model using Hyperparameter tuning. A hyperparameter is a parameter that we can set on a model before we can train it on the dataset. Can use tune_model function for tuning the hyperparameter of our model."""

#Tuning Light Gradient Boosting Model
tuned_LGBM = tune_model(LGBM)

"""Should using PyCaret for some visual aspects of the data"""

predict_model(tuned_LGBM)

#Finalize the model by restraining on the entire seen dataset
final_model = finalize_model(tuned_LGBM)
